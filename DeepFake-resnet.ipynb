{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DeepFake Detection"]},{"cell_type":"markdown","metadata":{},"source":["# 1: ResNet50 - Best Model"]},{"cell_type":"markdown","metadata":{},"source":["## Requirements"]},{"cell_type":"markdown","metadata":{},"source":["**To train the model**, first set the training and validation data paths in the constants section. Uncomment the training model part and run it to obtain the 'best_model' and reproduce the best model. The model weights will be saved in the current directory as 'best_model.pth.'\n","\n","**To test the model:**\n","\n","**Method 1:** set the testing data path in the constants section with 'test_data_path=...'. And go to model testing section. The model will load weights from 'best_model.pth' and run the test.\n","\n","**Method 2:** set the test txt path in the constants section with 'test_txt_path=...'. And go to model testing section. The model will load weights from 'best_model.pth' and run the test.\n","\n","**INSTALL**: pytorch, torchvision, sklearn"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:39:39.359241Z","iopub.status.busy":"2023-12-09T06:39:39.358889Z","iopub.status.idle":"2023-12-09T06:39:39.365632Z","shell.execute_reply":"2023-12-09T06:39:39.364600Z","shell.execute_reply.started":"2023-12-09T06:39:39.359216Z"},"trusted":true},"outputs":[],"source":["import os\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.datasets import ImageFolder\n","from torchvision.transforms import (\n","    Compose,\n","    ToTensor,\n","    Normalize,\n","    CenterCrop,\n","    Resize,\n","    RandomHorizontalFlip,\n","    RandomAffine,\n",")\n","\n","from torchvision.models import resnet50\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score"]},{"cell_type":"markdown","metadata":{},"source":["## Constants"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:39:41.675377Z","iopub.status.busy":"2023-12-09T06:39:41.674576Z","iopub.status.idle":"2023-12-09T06:39:41.681431Z","shell.execute_reply":"2023-12-09T06:39:41.680333Z","shell.execute_reply.started":"2023-12-09T06:39:41.675343Z"},"trusted":true},"outputs":[],"source":["# Data paths\n","train_data_path = \"./data/train/\"\n","val_data_path = \"./data/val/\"\n","\n","# Set on of the following\n","test_data_path = \"./data/test/\"\n","test_txt_path = \"./test.txt\"\n","\n","\n","# Set random seed and device\n","torch.manual_seed(42)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Batch size\n","batch_size = 32\n","\n","# Number of classes\n","num_classes = 2\n","\n","# Input size\n","input_size = (317, 317)\n","\n","# Number of epoch\n","num_epochs = 100"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset loader"]},{"cell_type":"markdown","metadata":{},"source":["We used os.walk() method to find our images as the data images are contanined in many directories."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:39:42.985917Z","iopub.status.busy":"2023-12-09T06:39:42.985263Z","iopub.status.idle":"2023-12-09T06:39:42.994728Z","shell.execute_reply":"2023-12-09T06:39:42.993805Z","shell.execute_reply.started":"2023-12-09T06:39:42.985885Z"},"trusted":true},"outputs":[],"source":["class MyDataset(Dataset):\n","    def __init__(self, data_path, transform=None):\n","        self.data_path = data_path\n","        self.transform = transform\n","        self.imgs = []\n","        self._load_data()\n","\n","    def _load_data(self):\n","        for root, dirs, _ in os.walk(self.data_path):\n","            for folder in dirs:\n","                folder_path = os.path.join(root, folder)\n","\n","                for file in os.listdir(folder_path):\n","                    file_path = os.path.join(folder_path, file)\n","\n","                    if file_path.endswith('jpg'):\n","                        label = 0 if \"Fake\" in folder_path else 1\n","                        self.imgs.append((file_path, label))\n","\n","    def __getitem__(self, index):\n","        img_path, label = self.imgs[index]\n","        img = Image.open(img_path).convert('RGB')\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.imgs)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T07:52:51.465577Z","iopub.status.busy":"2023-12-09T07:52:51.464652Z","iopub.status.idle":"2023-12-09T07:52:51.472814Z","shell.execute_reply":"2023-12-09T07:52:51.471869Z","shell.execute_reply.started":"2023-12-09T07:52:51.465541Z"},"trusted":true},"outputs":[],"source":["class MyDatasetTest(Dataset):\n","    def __init__(self, txt_path, transform=None):\n","        fh = open(txt_path, 'r')\n","        imgs = []\n","        for line in fh:\n","            line = line.rstrip()\n","            words = line.split()\n","            imgs.append((words[0], int(words[1])))\n","\n","        self.imgs = imgs\n","        self.transform = transform\n","\n","    def __getitem__(self, index):\n","        fn, label = self.imgs[index]\n","        img = Image.open(fn).convert('RGB')\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, label\n","\n","    def __len__(self):\n","        return len(self.imgs)"]},{"cell_type":"markdown","metadata":{},"source":["## ResNet50"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:44:27.037194Z","iopub.status.busy":"2023-12-09T06:44:27.036785Z","iopub.status.idle":"2023-12-09T06:44:27.506150Z","shell.execute_reply":"2023-12-09T06:44:27.505218Z","shell.execute_reply.started":"2023-12-09T06:44:27.037165Z"},"trusted":true},"outputs":[],"source":["# Model\n","class ResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(ResNet, self).__init__()\n","        self.features = resnet50(pretrained=True)\n","        \n","        # Freeze all layers except the last three\n","        for param in self.features.parameters():\n","            param.requires_grad = False\n","            \n","        # Modify the last three layers\n","        self.features.layer3.requires_grad_(True)\n","        self.features.layer4.requires_grad_(True)\n","        self.features.fc = nn.Linear(2048, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        return x\n","\n","# Initialize model, criterion, and optimizer\n","model = ResNet(num_classes=num_classes)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:48:38.794991Z","iopub.status.busy":"2023-12-09T06:48:38.794061Z","iopub.status.idle":"2023-12-09T06:48:38.800934Z","shell.execute_reply":"2023-12-09T06:48:38.799724Z","shell.execute_reply.started":"2023-12-09T06:48:38.794957Z"},"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:49:07.673695Z","iopub.status.busy":"2023-12-09T06:49:07.673281Z","iopub.status.idle":"2023-12-09T06:49:07.680100Z","shell.execute_reply":"2023-12-09T06:49:07.679109Z","shell.execute_reply.started":"2023-12-09T06:49:07.673666Z"},"trusted":true},"outputs":[],"source":["def train_epoch(model, data_loader, criterion, optimizer, device):\n","    model.train()\n","    train_loss = 0.0\n","\n","    for images, labels in data_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(images)\n","\n","        # Calculate loss\n","        loss = criterion(outputs, labels)\n","        train_loss += loss.item()\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","    train_loss /= len(data_loader)\n","    return train_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:49:08.661506Z","iopub.status.busy":"2023-12-09T06:49:08.661150Z","iopub.status.idle":"2023-12-09T06:49:08.671650Z","shell.execute_reply":"2023-12-09T06:49:08.670713Z","shell.execute_reply.started":"2023-12-09T06:49:08.661476Z"},"trusted":true},"outputs":[],"source":["def validate(model, data_loader, criterion, device):\n","    model.eval()\n","    val_loss = 0.0\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for images, labels in data_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","\n","            # Calculate loss\n","            loss = criterion(outputs, labels)\n","            val_loss += loss.item()\n","\n","            # Calculate predictions\n","            predictions = torch.argmax(outputs, dim=1)\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predictions.cpu().numpy())\n","\n","    val_loss /= len(data_loader)\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","    return val_loss, accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T06:49:21.331266Z","iopub.status.busy":"2023-12-09T06:49:21.330601Z","iopub.status.idle":"2023-12-09T06:49:21.340862Z","shell.execute_reply":"2023-12-09T06:49:21.339817Z","shell.execute_reply.started":"2023-12-09T06:49:21.331230Z"},"trusted":true},"outputs":[],"source":["def train_model(model, train_path, val_path, transform=None, target_transform=None, num_epochs=100, batch_size=32):\n","    # Datasets\n","    train_dataset = MyDataset(train_path, transform=transform)\n","    val_dataset = MyDataset(val_path, transform=target_transform)\n","\n","    # Data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","    \n","    best_val_accuracy = 0\n","    \n","    for epoch in range(num_epochs):\n","        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n","        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n","\n","        # Save best model based on validation accuracy\n","        if val_accuracy > best_val_accuracy:\n","            best_val_accuracy = val_accuracy\n","            torch.save(model.state_dict(), 'best_model.pth')\n","\n","        # Print training and validation metrics for each epoch\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}]\\tTrain Loss: {train_loss:.4f}\\tVal Loss: {val_loss:.4f}\\tVal Accuracy: {val_accuracy:.4f}\")\n","\n","        with open('train_losses.txt', 'a') as f:\n","            f.write(f\"{train_loss}\\n\")\n","\n","        with open('val_losses.txt', 'a') as f:\n","            f.write(f\"{val_loss}\\n\")\n","\n","        with open('val_accuracies.txt', 'a') as f:\n","            f.write(f\"{val_accuracy}\\n\")\n","    \n","    return best_val_accuracy, model"]},{"cell_type":"markdown","metadata":{},"source":["## Test Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T08:12:52.481868Z","iopub.status.busy":"2023-12-09T08:12:52.481459Z","iopub.status.idle":"2023-12-09T08:12:52.487776Z","shell.execute_reply":"2023-12-09T08:12:52.486696Z","shell.execute_reply.started":"2023-12-09T08:12:52.481837Z"},"trusted":true},"outputs":[],"source":["def load_test_data(data_path, transform, isTxt = 0):\n","    if isTxt:\n","        test_dataset = MyDatasetTest(data_path, transform=transform)\n","    else:\n","        test_dataset = MyDataset(data_path, transform=transform)\n","        \n","    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","    return test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T08:12:53.230806Z","iopub.status.busy":"2023-12-09T08:12:53.230433Z","iopub.status.idle":"2023-12-09T08:12:53.239006Z","shell.execute_reply":"2023-12-09T08:12:53.237922Z","shell.execute_reply.started":"2023-12-09T08:12:53.230775Z"},"trusted":true},"outputs":[],"source":["def test(model, data_path, transform, isTxt = 0):\n","    test_loader = load_test_data(data_path, transform, isTxt)\n","    \n","    model.eval()\n","    \n","    accuracy, recall, precision, auc = 0, 0, 0, 0\n","    \n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for images, labels in test_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            # Forward pass\n","            outputs = model(images)\n","\n","            # Calculate predictions\n","            predictions = get_predictions(outputs)\n","            \n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predictions.cpu().numpy())\n","            \n","    # Calculate final metrics\n","    accuracy = accuracy_score(all_labels, all_predictions)\n","    recall = recall_score(all_labels, all_predictions)\n","    precision = precision_score(all_labels, all_predictions)\n","    auc = roc_auc_score(all_labels, all_predictions)\n","\n","    return accuracy, recall, precision, auc"]},{"cell_type":"markdown","metadata":{},"source":["## Training & Testing "]},{"cell_type":"markdown","metadata":{},"source":["### Transformations\n","\n","**Augmentations**, we normalize all the images according to the the default values of mean and standard deviation of pythorch. The training images are further processed by appliying random horizontal flips, random shear and scale. The images are also resized to (317,317). Exact details can be seen below:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T08:12:54.607414Z","iopub.status.busy":"2023-12-09T08:12:54.606263Z","iopub.status.idle":"2023-12-09T08:12:54.613698Z","shell.execute_reply":"2023-12-09T08:12:54.612819Z","shell.execute_reply.started":"2023-12-09T08:12:54.607380Z"},"trusted":true},"outputs":[],"source":["normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","\n","train_transform = Compose([\n","    Resize((317, 317)),\n","    CenterCrop((317, 317)),\n","    RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n","    RandomHorizontalFlip(),\n","    ToTensor(),\n","    normalize\n","])\n","\n","val_transform = Compose([\n","    Resize((317, 317)),\n","    ToTensor(),\n","    normalize\n","])\n","\n","test_transform = Compose([\n","    Resize((317, 317)),\n","    ToTensor(),\n","    normalize\n","])"]},{"cell_type":"markdown","metadata":{},"source":["### Training model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T08:12:55.794046Z","iopub.status.busy":"2023-12-09T08:12:55.793614Z","iopub.status.idle":"2023-12-09T08:12:55.798009Z","shell.execute_reply":"2023-12-09T08:12:55.797122Z","shell.execute_reply.started":"2023-12-09T08:12:55.794015Z"},"trusted":true},"outputs":[],"source":["best_accuracy, best_model = train_model(model, train_data_path, val_data_path, transform=train_transform, target_transform=val_transform, num_epochs=100, batch_size=32)\n","print(\"Best validation accuracy: \" + best_accuracy)"]},{"cell_type":"markdown","metadata":{},"source":["### Testing model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-09T08:12:57.197833Z","iopub.status.busy":"2023-12-09T08:12:57.197460Z","iopub.status.idle":"2023-12-09T08:12:57.754972Z","shell.execute_reply":"2023-12-09T08:12:57.753870Z","shell.execute_reply.started":"2023-12-09T08:12:57.197804Z"},"trusted":true},"outputs":[],"source":["best_model = ResNet(num_classes=2)\n","best_model.to(device)\n","best_model.load_state_dict(torch.load('/kaggle/input/best-model/best_model.pth'))"]},{"cell_type":"markdown","metadata":{},"source":["### Method 1: Data in directory"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T08:14:02.981391Z","iopub.status.busy":"2023-12-09T08:14:02.980612Z","iopub.status.idle":"2023-12-09T08:20:08.417056Z","shell.execute_reply":"2023-12-09T08:20:08.416154Z","shell.execute_reply.started":"2023-12-09T08:14:02.981357Z"},"trusted":true},"outputs":[],"source":["accuracy, recall, precision, auc = test(best_model, test_data_path, test_transform)\n","\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","print(f\"Test Recall: {recall:.4f}\")\n","print(f\"Test Precision: {precision:.4f}\")\n","print(f\"Test AUC: {auc:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Method 2: Img paths in .txt file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accuracy, recall, precision, auc = test(best_model, test_txt_path, test_transform, isTxt = 1)\n","\n","print(f\"Test Accuracy: {accuracy:.4f}\")\n","print(f\"Test Recall: {recall:.4f}\")\n","print(f\"Test Precision: {precision:.4f}\")\n","print(f\"Test AUC: {auc:.4f}\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4130423,"sourceId":7153168,"sourceType":"datasetVersion"},{"datasetId":4134598,"sourceId":7158938,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
